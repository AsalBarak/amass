{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using AMASS for training DNNs for Human Body and Motion\n",
    "Just like [ImageNet](http://www.image-net.org/), AMASS is huge database ready to enable proper deep learning. However, instead of images, it consists of human body parameters, controlling the surface mesh of the [SMPL+H body model](http://mano.is.tue.mpg.de/). \n",
    "\n",
    "Here we provide basic tools to turn compressed AMASS numpy arrays that are as *.npz* files, into other suitable formats for deep learning frameworks. The outcomes are PyTorch readable *.pt* files, as well as *.h5* files\n",
    "that are commonly used in machine learning fields. \n",
    "\n",
    "The provided data preparation code has three stages that could be flexibly modified for your own specific needs.\n",
    "\n",
    "**Stage I** goes over the previously downloaded numpy npz files, subsamples them and saves them all into one place as PyTorch pt files.\n",
    "\n",
    "**Stage II** uses PyTorch to apply all sorts of data augmentations \n",
    "in parallel on the original data and saves them as final HDF5 files.\n",
    "HDF5 makes it possible to write on the file in chunks so that you wouldn't run out of memory\n",
    "if you do a huge number of data augmentation steps. PyTorch also handles data augmentation in parallel so you should achieve your augmentation goal as fast as possible. \n",
    "\n",
    "**Stage III** simply turns the h5 files into pt files to be readily usable by PyTorch.\n",
    "\n",
    "The progress at all stages is logged and could be inspected at any time during the process or later.\n",
    "We also use an experiment ID that helps in referring to a specific data preparation run that can be traced back."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dependencies to run this notebook:\n",
    "- [Human Body Prior](https://github.com/nghorbani/human_body_prior)\n",
    "- [PyTorch>=1.1.0](https://pytorch.org/)\n",
    "- [PyTables](https://www.pytables.org/usersguide/installation.html)\n",
    "- [tqdm](https://pypi.org/project/tqdm/2.2.3/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before you continue with this tutorial, it is recommended to first take a look at the [**AMASS Visualization**](01-AMASS_Visualization.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the environment\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "\n",
    "import sys, os\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from human_body_prior.tools.omni_tools import makepath, log2file\n",
    "from human_body_prior.tools.omni_tools import copy2cpu as c2c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first pick an experiment ID that later helps us to identify the specific run of augmentations and data preparation. We also prepare a message about this specific experiment, that is going to be included in our log file as future notes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "expr_code = 'VXX_SVXX_TXX' #VERSION_SUBVERSION_TRY\n",
    "\n",
    "msg = ''' Initial use of standard AMASS dataset preparation pipeline '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then you have to download the body data npz files from https://amass.is.tue.mpg.de/datasets. Uncompress them at your desired location. The final folder structure should be like the following:\n",
    "\n",
    "**amass_dir>sub_dataset>subjects>*_poses.npz**\n",
    "\n",
    "Now specify the directory of the downloaded npz files and the final output folder for the prepared data, e.g. **work_dir**. At this location at each stage of the data processing pipeline, an appropriate folder is created. Moreover, the log file and a snapshot of the code is created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[VXX_SVXX_TXX] AMASS Data Preparation Began.\n",
      " Initial use of standard AMASS dataset preparation pipeline \n"
     ]
    }
   ],
   "source": [
    "amass_dir = 'PATH_TO_DOWNLOADED_NPZFILES/*/*_poses.npz'\n",
    "\n",
    "work_dir = makepath('WHERE_YOU_WANT_YOUR_FILE_TO_BE_DUMPED/%s' % (expr_code))\n",
    "\n",
    "logger = log2file(os.path.join(work_dir, '%s.log' % (expr_code)))\n",
    "logger('[%s] AMASS Data Preparation Began.'%expr_code)\n",
    "logger(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you have to specify data splits for train/validation/test. Below is the recommended data splits by AMASS\n",
    "for train/validation/test that chooses data for each split from non-overlapping datasets. \n",
    "One reason is to avoid similarity of markersets used for the original data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amass_splits = {\n",
    "    'vald': ['HumanEva', 'MPI_HDM05', 'SFU', 'MPI_mosh'],\n",
    "    'test': ['Transitions_mocap', 'SSM_synced'],\n",
    "    'train': ['CMU', 'MPI_Limits', 'TotalCapture', 'Eyes_Japan_Dataset', 'KIT', \n",
    "              'BML', 'EKUT', 'TCD_handMocap', 'ACCAD']\n",
    "}\n",
    "amass_splits['train'] = list(set(amass_splits['train']).difference(set(amass_splits['test'] + amass_splits['vald'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have tools in place to run the preparation code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Stage I: Fetch data from AMASS npz files\n",
      "randomly selecting data points from HumanEva.\n",
      "100%|██████████| 28/28 [00:00<00:00, 60.86it/s]\n",
      "randomly selecting data points from MPI_HDM05.\n",
      "100%|██████████| 215/215 [00:07<00:00, 30.27it/s]\n",
      "randomly selecting data points from SFU.\n",
      "100%|██████████| 44/44 [00:00<00:00, 51.51it/s]\n",
      "randomly selecting data points from MPI_mosh.\n",
      "100%|██████████| 77/77 [00:00<00:00, 75.23it/s]\n",
      "randomly selecting data points from Transitions_mocap.\n",
      "100%|██████████| 110/110 [00:01<00:00, 96.11it/s]\n",
      "randomly selecting data points from SSM_synced.\n",
      "100%|██████████| 30/30 [00:00<00:00, 196.17it/s]\n",
      "randomly selecting data points from MPI_Limits.\n",
      "100%|██████████| 35/35 [00:01<00:00, 34.75it/s]\n",
      "randomly selecting data points from Eyes_Japan_Dataset.\n",
      " 15%|█▌        | 114/750 [00:21<02:36,  4.06it/s]"
     ]
    }
   ],
   "source": [
    "from amass.prepare_data import prepare_amass\n",
    "prepare_amass(amass_splits, amass_dir, work_dir, logger=logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the prepared data\n",
    "Now that final PyTorch .pt files are prepared we can try to open them with PyTorch wich a dataloader:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import glob \n",
    "\n",
    "class AMASS_DS(Dataset):\n",
    "    \"\"\"AMASS: a pytorch loader for unified human motion capture dataset. http://amass.is.tue.mpg.de/\"\"\"\n",
    "\n",
    "    def __init__(self, dataset_dir, num_betas=16):\n",
    "\n",
    "        self.ds = {}\n",
    "        for data_fname in glob.glob(os.path.join(dataset_dir, '*.pt')):\n",
    "            k = os.path.basename(data_fname).replace('.pt','')\n",
    "            self.ds[k] = torch.load(data_fname)\n",
    "        self.num_betas = num_betas\n",
    "\n",
    "    def __len__(self):\n",
    "       return len(self.ds['trans'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data =  {k: self.ds[k][idx] for k in self.ds.keys()}\n",
    "        data['root_orient'] = data['pose'][:3]\n",
    "        data['pose_body'] = data['pose'][3:66]\n",
    "        data['pose_hand'] = data['pose'][66:]\n",
    "        data['betas'] = data['betas'][:self.num_betas]\n",
    "\n",
    "        return data\n",
    "\n",
    "num_betas = 16 # number of body parameters\n",
    "testsplit_dir = os.path.join(work_dir, 'stage_III', 'test')\n",
    "\n",
    "ds = AMASS_DS(dataset_dir=testsplit_dir, num_betas=num_betas)\n",
    "print('Test split has %d datapoints.'%len(ds))\n",
    "\n",
    "batch_size = 5\n",
    "dataloader = DataLoader(ds, batch_size=batch_size, shuffle=True, num_workers=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the tutorial in [**AMASS Visualization**](01-AMASS_Visualization.ipynb) to visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import trimesh\n",
    "from human_body_prior.tools.omni_tools import colors\n",
    "from human_body_prior.mesh import MeshViewer\n",
    "from human_body_prior.mesh.sphere import points_to_spheres\n",
    "from human_body_prior.tools.omni_tools import apply_mesh_tranfsormations_\n",
    "from human_body_prior.tools.visualization_tools import imagearray2file\n",
    "from notebook_tools import show_image\n",
    "\n",
    "imw, imh=1600, 1600\n",
    "mv = MeshViewer(width=imw, height=imh, use_offscreen=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from human_body_prior.body_model.body_model import BodyModel\n",
    "bm_path = '../body_models/smplh/male/model.npz'\n",
    "\n",
    "bm = BodyModel(bm_path=bm_path, num_betas=num_betas, batch_size=batch_size)\n",
    "faces = c2c(bm.f)\n",
    "\n",
    "bdata = next(iter(dataloader))\n",
    "body_v = bm.forward(pose_body=bdata['pose_body'], betas=bdata['betas']).v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_angles = [0, 180, 90, -90]\n",
    "images = np.zeros([len(view_angles), batch_size, 1, imw, imh, 3])\n",
    "for cId in range(0, batch_size):\n",
    "\n",
    "    orig_body_mesh = trimesh.Trimesh(vertices=c2c(body_v[cId]), faces=c2c(bm.f), vertex_colors=np.tile(colors['grey'], (6890, 1)))\n",
    "\n",
    "    for rId, angle in enumerate(view_angles):\n",
    "        if angle != 0: apply_mesh_tranfsormations_([orig_body_mesh], trimesh.transformations.rotation_matrix(np.radians(angle), (0, 1, 0)))\n",
    "        mv.set_meshes([orig_body_mesh], group_name='static')\n",
    "        images[rId, cId, 0] = mv.render()\n",
    "\n",
    "        if angle != 0: apply_mesh_tranfsormations_([orig_body_mesh], trimesh.transformations.rotation_matrix(np.radians(-angle), (0, 1, 0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = imagearray2file(images)\n",
    "show_image(np.array(img)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above image each column is a data point and at each row we rotate the body to visualize it in different angles."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
